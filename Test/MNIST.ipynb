{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T08:32:57.545333Z",
     "start_time": "2024-11-28T08:32:57.541816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'"
   ],
   "id": "f3ed095b53e94db7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T09:05:30.842190Z",
     "start_time": "2024-11-28T09:05:29.417779Z"
    }
   },
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 17:05:29.803195: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732784729.812104 1086326 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732784729.814582 1086326 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:49:32.179771Z",
     "start_time": "2024-11-28T09:49:32.178275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @markdown The learning rate for the optimizer:\n",
    "LEARNING_RATE = 0.002 # @param{type:\"number\"}\n",
    "# @markdown Number of samples in each batch:\n",
    "BATCH_SIZE = 128 # @param{type:\"integer\"}\n",
    "# @markdown Total number of epochs to train for:\n",
    "N_EPOCHS = 5 # @param{type:\"integer\"}"
   ],
   "id": "8e4804f9f7f9feb9",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T09:49:54.166106Z",
     "start_time": "2024-11-28T09:49:54.100129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "(train_loader, test_loader), info = tfds.load(\n",
    "    \"mnist\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n",
    ")\n",
    "\n",
    "min_max_rgb = lambda image, label: (tf.cast(image, tf.float32) / 255., label)\n",
    "train_loader = train_loader.map(min_max_rgb)\n",
    "test_loader = test_loader.map(min_max_rgb)\n",
    "\n",
    "NUM_CLASSES = info.features[\"label\"].num_classes\n",
    "IMG_SIZE = info.features[\"image\"].shape\n",
    "\n",
    "train_loader_batched = train_loader.shuffle(\n",
    "    buffer_size=10000, reshuffle_each_iteration=True\n",
    ").batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "test_loader_batched = test_loader.batch(BATCH_SIZE, drop_remainder=True)"
   ],
   "id": "5194d29310b73fc6",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:55:33.670080Z",
     "start_time": "2024-11-29T02:55:33.666268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple multilayer perceptron model for image classification.\"\"\"\n",
    "    hidden_sizes: Sequence[int] = (100, 100)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Flatten the input images\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=self.hidden_sizes[0])(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.hidden_sizes[1])(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=NUM_CLASSES)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    \"A simple LeNet model for image classification.\"\n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool):\n",
    "        x = nn.Conv(features=6, kernel_size=(5, 5))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = nn.Conv(features=16, kernel_size=(5, 5))(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nn.Dense(features=120)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.Dropout(rate=0.25, deterministic=not train)(x)\n",
    "        x = nn.Dense(features=84)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=NUM_CLASSES)(x)\n",
    "        return x"
   ],
   "id": "d82ed8129f7c7c57",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:55:35.341187Z",
     "start_time": "2024-11-29T02:55:35.339265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# net = MLP()\n",
    "net = LeNet()"
   ],
   "id": "bbee3877bcb75fa4",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:52:52.489870Z",
     "start_time": "2024-11-29T02:52:52.487139Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def predict(params, inputs, batch_states):\n",
    "    return net.apply({\"params\": params, \"batch_stats\": batch_states}, inputs, train=True)\n",
    "\n",
    "def loss_accuracy(params, data):\n",
    "    \"\"\"Computes loss and accuracy over a mini-batch.\n",
    "\n",
    "    Args:\n",
    "        params: the model parameters\n",
    "        data: a tuple of (images, labels)\n",
    "    Returns:\n",
    "        loss: the average loss over the mini-batch (float)\n",
    "    \"\"\"\n",
    "\n",
    "    inputs, labels = data\n",
    "    logits, updates = predict(params, inputs)\n",
    "    batch_stats = updates['batch_stats']\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels).mean()\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "    return loss, {\"accuracy\": accuracy}\n",
    "\n",
    "@jax.jit\n",
    "def update_model(state, grads):\n",
    "    return state.apply_gradients(grads=grads)"
   ],
   "id": "4fac4cbe9f2925f7",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:52:57.063204Z",
     "start_time": "2024-11-29T02:52:57.007375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "solver = optax.adam(LEARNING_RATE)\n",
    "rng1 = jax.random.PRNGKey(0)\n",
    "rng2 = jax.random.PRNGKey(1)\n",
    "dummy_data = jnp.ones((1,)+IMG_SIZE, dtype=jnp.float32)\n",
    "vars = net.init({\"params\": rng1, \"\"}, dummy_data, train=False)\n",
    "params = vars[\"params\"]\n",
    "\n",
    "solver_state = solver.init(params)\n",
    "\n",
    "def dataset_stats(params, data_loader):\n",
    "    \"\"\"Compute the loss and accuracy over a dataset.\"\"\"\n",
    "    all_accuracy = []\n",
    "    all_loss = []\n",
    "    for batch in data_loader.as_numpy_iterator():\n",
    "        batch_loss, batch_aux = loss_accuracy(params, batch)\n",
    "        all_loss.append(batch_loss)\n",
    "        all_accuracy.append(batch_aux['accuracy'])\n",
    "\n",
    "    return {\"loss\": np.mean(all_loss), \"accuracy\": np.mean(all_accuracy)}"
   ],
   "id": "32feed5739f10c1",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:52:59.908287Z",
     "start_time": "2024-11-29T02:52:59.855976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_accuracy = []\n",
    "train_losses = []\n",
    "\n",
    "# Compute test set accuracy before training\n",
    "test_stats = dataset_stats(params, test_loader_batched)\n",
    "test_accuracy = [test_stats[\"accuracy\"]]\n",
    "test_losses = [test_stats[\"loss\"]]\n",
    "\n",
    "@jax.jit\n",
    "def train_step(params, solver_state, batch):\n",
    "    # performs a one-step update, aux is the accuracy\n",
    "    (loss, aux), grad = jax.value_and_grad(loss_accuracy, has_aux=True)(params, batch, solver_state)\n",
    "    updates, solver_state = solver.update(grad, solver_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, solver_state, loss, aux\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_accuracy_epoch = []\n",
    "    train_losses_epoch = []\n",
    "\n",
    "    for step, train_batch in enumerate(train_loader_batched.as_numpy_iterator()):\n",
    "        params, solver_state, loss, aux = train_step(params, solver_state, train_batch)\n",
    "        train_accuracy_epoch.append(aux['accuracy'])\n",
    "        train_losses_epoch.append(loss)\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step:<4}, Loss: {loss:<8.4e}, Accuracy: {aux['accuracy']:<5.2f}\")\n",
    "\n",
    "    test_stats = dataset_stats(params, test_loader_batched)\n",
    "    test_accuracy.append(test_stats[\"accuracy\"])\n",
    "    test_losses.append(test_stats[\"loss\"])\n",
    "    train_accuracy.append(np.mean(train_accuracy_epoch))\n",
    "    train_losses.append(np.mean(train_losses_epoch))\n",
    "\n",
    "    print(f\"Epoch {epoch:<4}, Loss: {train_losses[-1]:<8.4e}, Accuracy: {train_accuracy[-1]:<5.2f}, Test Accuracy: {test_accuracy[-1]:<5.2f}\")"
   ],
   "id": "ddb05bb137da93df",
   "outputs": [
    {
     "ename": "InvalidRngError",
     "evalue": "Dropout_0 needs PRNG for \"dropout\" (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.InvalidRngError)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidRngError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[75], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m train_losses \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Compute test set accuracy before training\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m test_stats \u001B[38;5;241m=\u001B[39m dataset_stats(params, test_loader_batched)\n\u001B[1;32m      6\u001B[0m test_accuracy \u001B[38;5;241m=\u001B[39m [test_stats[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[1;32m      7\u001B[0m test_losses \u001B[38;5;241m=\u001B[39m [test_stats[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n",
      "Cell \u001B[0;32mIn[74], line 14\u001B[0m, in \u001B[0;36mdataset_stats\u001B[0;34m(params, data_loader)\u001B[0m\n\u001B[1;32m     12\u001B[0m all_loss \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m data_loader\u001B[38;5;241m.\u001B[39mas_numpy_iterator():\n\u001B[0;32m---> 14\u001B[0m     batch_loss, batch_aux \u001B[38;5;241m=\u001B[39m loss_accuracy(params, batch)\n\u001B[1;32m     15\u001B[0m     all_loss\u001B[38;5;241m.\u001B[39mappend(batch_loss)\n\u001B[1;32m     16\u001B[0m     all_accuracy\u001B[38;5;241m.\u001B[39mappend(batch_aux[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "Cell \u001B[0;32mIn[73], line 16\u001B[0m, in \u001B[0;36mloss_accuracy\u001B[0;34m(params, data)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Computes loss and accuracy over a mini-batch.\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m    loss: the average loss over the mini-batch (float)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     15\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m---> 16\u001B[0m logits, updates \u001B[38;5;241m=\u001B[39m predict(params, inputs)\n\u001B[1;32m     17\u001B[0m batch_stats \u001B[38;5;241m=\u001B[39m updates[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_stats\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m optax\u001B[38;5;241m.\u001B[39msoftmax_cross_entropy_with_integer_labels(logits\u001B[38;5;241m=\u001B[39mlogits, labels\u001B[38;5;241m=\u001B[39mlabels)\u001B[38;5;241m.\u001B[39mmean()\n",
      "    \u001B[0;31m[... skipping hidden 11 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[73], line 3\u001B[0m, in \u001B[0;36mpredict\u001B[0;34m(params, inputs)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;129m@jax\u001B[39m\u001B[38;5;241m.\u001B[39mjit\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(params, inputs):\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m net\u001B[38;5;241m.\u001B[39mapply({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m\"\u001B[39m: params}, inputs, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "    \u001B[0;31m[... skipping hidden 6 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[70], line 29\u001B[0m, in \u001B[0;36mLeNet.__call__\u001B[0;34m(self, x, train)\u001B[0m\n\u001B[1;32m     27\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mreshape((x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m     28\u001B[0m x \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDense(features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m120\u001B[39m)(x)\n\u001B[0;32m---> 29\u001B[0m x \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m, deterministic\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m train)(x)\n\u001B[1;32m     30\u001B[0m x \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m     31\u001B[0m x \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDense(features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m84\u001B[39m)(x)\n",
      "    \u001B[0;31m[... skipping hidden 2 frame]\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/jax311/lib/python3.11/site-packages/flax/linen/stochastic.py:101\u001B[0m, in \u001B[0;36mDropout.__call__\u001B[0;34m(self, inputs, deterministic, rng)\u001B[0m\n\u001B[1;32m     99\u001B[0m keep_prob \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrate\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m rng \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 101\u001B[0m   rng \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_rng(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrng_collection)\n\u001B[1;32m    102\u001B[0m broadcast_shape \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(inputs\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dim \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbroadcast_dims:\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "File \u001B[0;32m~/anaconda3/envs/jax311/lib/python3.11/site-packages/flax/core/scope.py:757\u001B[0m, in \u001B[0;36mScope.make_rng\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    755\u001B[0m     name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    756\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 757\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m errors\u001B[38;5;241m.\u001B[39mInvalidRngError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m needs PRNG for \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    758\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_valid()\n\u001B[1;32m    759\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_trace_level()\n",
      "\u001B[0;31mInvalidRngError\u001B[0m: Dropout_0 needs PRNG for \"dropout\" (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.InvalidRngError)"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:42:28.556842Z",
     "start_time": "2024-11-29T02:42:28.554690Z"
    }
   },
   "cell_type": "code",
   "source": "f\"Improved accuracy on test DS from {test_accuracy[0]} to {test_accuracy[-1]}\"",
   "id": "176a4e71e0139fbe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Improved accuracy on test DS from 0.10136217623949051 to 0.9888821840286255'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0427394802b92c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
